---
title: "Chap15_July13_July20_2020"
author: "Kazu"
date: "7/5/2020"
output: 
  html_document: 
    keep_md: yes
editor_options: 
  chunk_output_type: console
---



# Practice (July 20)
PDF #1 and 2 https://github.com/rmcelreath/statrethinking_winter2019/blob/master/homework/week10.pdf
* 1. Consider the relationship between brain volume (brain) and bodymass(body) in the data(Primates301). These values are presented as single values for each species. However, there is always a range of sizes in a species, and some of these measurements are taken from very small samples. So these values are measured with some unknown error.
We don’t have the raw measurements to work with—that would be best. But we can imagine what might happen if we had them. Suppose error is proportional to the measurement. This makes sense, because larger animals have larger variation. As a consequence, the uncertainty is not uniform across the values and this could mean trouble.
Let’s make up some standard errors for these measurements, to see what might happen. Load the data and scale the the measurements so the maximum is 1 in both cases:

```r
data(Primates301)
d <- Primates301
cc <- complete.cases( d$brain , d$body )
B <- d$brain[cc]
M <- d$body[cc]
B <- B / max(B)
M <- M / max(M)
```
* Now I’ll make up some standard errors for B and M, assuming error is 10% of the measurement.

```r
Bse <- B*0.1
Mse <- M*0.1
```
* Let’s model these variables with this relationship:
* Bi ∼ Log-Normal(μi, σ)
* μi =α+βlogMi 1

* This says that brain volume is a log-normal variable, and the mean on the log scale is given by μ. What this model implies is that the expected value of B is:
* E(Bi|Mi) = exp(α)Mβi
* So this is a standard allometric scaling relationship—incredibly common in biology.
* Ignoring measurement error, the corresponding ulam model is:

```r
dat_list <- list(
    B = B,
M=M)
m1.1 <- ulam(
    alist(
        B ~ dlnorm( mu , sigma ),
        mu <- a + b*log(M),
        a ~ normal(0,1),
        b ~ normal(0,1),
        sigma ~ exponential(1)
    ) , data=dat_list,chains=4,cores=2,iter=2000)
```
* Your job is to add the measurement errors to this model. Use the divorce/marriage example in the chapter as a guide. It might help to initialize the unobserved true values of B and M using the observed values, by adding a list like this to ulam:

```r
 start=list( M_true=dat_list$M , B_true=dat_list$B,log_lik = T)
```

# Answer (Kazu)

```r
# dlist <- list(
#     D_obs = standardize( d$Divorce ),
#     D_sd = d$Divorce.SE / sd( d$Divorce ),
#     M_obs = standardize( d$Marriage ),
#     M_sd = d$Marriage.SE / sd( d$Marriage ),
#     A = standardize( d$MedianAgeMarriage ),
#     N = nrow(d)
# )
# 
# m15.2 <- ulam(
#     alist(
#         D_obs ~ dnorm( D_true , D_sd ),
#         vector[N]:D_true ~ dnorm( mu , sigma ),
#         mu <- a + bA*A + bM*M_true[i],
#         M_obs ~ dnorm( M_true , M_sd ),
#         vector[N]:M_true ~ dnorm( 0 , 1 ),
#         a ~ dnorm(0,0.2),
#         bA ~ dnorm(0,0.5),
#         bM ~ dnorm(0,0.5),
#         sigma ~ dexp( 1 )
#     ) , data=dlist , chains=4 , cores=4 )
dat_list$M_sd <- Mse/sd(dat_list$M)
dat_list$B_sd <- Bse/sd(dat_list$B)
dat_list$N <- sum(cc)

m1.2 <- ulam(
    alist(
        B_obs ~ dnorm(B_true,B_sd),
        vector[N]:B_true ~ dlnorm( mu , sigma ),
        mu <- a + b*log(M_true[i]),
        M_obs ~dnorm(M_true,M_sd),
        vector[N]:M_true ~ dnorm(0,1),
        a ~ normal(0,1),
        b ~ normal(0,1),
        sigma ~ exponential(1)
    ) , data=dat_list,
    start=list( M_true=dat_list$M , B_true=dat_list$B ),
    chains=4,cores=2,iter=5000)
```

```
## Warning: There were 620 divergent transitions after warmup. Increasing adapt_delta above 0.95 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
```

```
## Warning: There were 8063 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded
```

```
## Warning: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See
## http://mc-stan.org/misc/warnings.html#bfmi-low
```

```
## Warning: Examine the pairs() plot to diagnose sampling problems
```

```
## Warning: The largest R-hat is 2.99, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#r-hat
```

```
## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess
```

```
## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess
```

```r
# "log_lik = T" gave me " no parameter log_lik; sampling not done"
```

* Why following does not work?

```r
dat_list2 <- list(
  M_true=dat_list$M, 
  B_true=dat_list$B,
  M_sd = Mse/sd(dat_list$M),
  B_sd = Bse/sd(dat_list$B),
  N = sum(cc)
  )
m1.3 <- ulam(
    alist(
        B_obs ~ dnorm(B_true,B_sd),
        vector[N]:B_true ~ dlnorm( mu , sigma ),
        mu <- a + b*log(M_true[i]),
        M_obs ~dnorm(M_true,M_sd),
        vector[N]:M_true ~ dnorm(0,1),
        a ~ normal(0,1),
        b ~ normal(0,1),
        sigma ~ exponential(1)
    ) , data=dat_list2,
    chains=4,cores=2,iter=5000)
# "log_lik = T" gave me " no parameter log_lik; sampling not done"
```


* Compare the inference of the measurement error model to those of m1.1 above. Has anything changed? Why or why not?

```r
precis( m1.1 , depth=3 )
```

```
##            mean         sd      5.5%     94.5%    n_eff    Rhat4
## a     0.4262940 0.05828876 0.3305629 0.5181112 1383.088 1.004554
## b     0.7831949 0.01415281 0.7602786 0.8058325 1391.861 1.005429
## sigma 0.2931529 0.01577602 0.2688456 0.3189973 2049.293 1.001610
```

```r
precis( m1.2 , depth=3 )
```

```
##                    mean          sd          5.5%      94.5%      n_eff
## B_obs        0.43522585 0.690013654  9.320870e-03 1.65335671   2.001856
## B_true[1]    0.08392060 0.033517893  4.291930e-02 0.11811402   2.041003
## B_true[2]    0.07888792 0.029366295  4.026633e-02 0.10875389   2.085107
## B_true[3]    0.07978252 0.028907270  4.245447e-02 0.10996090   2.087296
## B_true[4]    0.07819997 0.027576206  4.249461e-02 0.10650978   2.072297
## B_true[5]    0.07632929 0.026866639  3.923039e-02 0.10376730   2.107753
## B_true[6]    0.07897076 0.027390755  3.948493e-02 0.10653605   2.200331
## B_true[7]    0.08528111 0.035344620  3.943802e-02 0.12026939   2.073183
## B_true[8]    0.08139268 0.030848571  4.166456e-02 0.11240873   2.058594
## B_true[9]    0.04689001 0.005386456  4.207168e-02 0.05659382   3.099961
## B_true[10]   0.04204131 0.009590192  3.249465e-02 0.05644843   2.208254
## B_true[11]   0.04288685 0.008812246  3.428881e-02 0.05650357   2.267514
## B_true[12]   0.03163842 0.019779434  1.196871e-02 0.05636413   2.042689
## B_true[13]   0.03252222 0.018853281  1.363042e-02 0.05641249   2.047666
## B_true[14]   0.14385645 0.092835271  3.727418e-02 0.23820064   2.017561
## B_true[15]   0.14064765 0.090243050  3.871026e-02 0.23350512   2.018128
## B_true[16]   0.13101449 0.081351415  3.692758e-02 0.21390819   2.017671
## B_true[17]   0.12987207 0.079254953  3.957783e-02 0.21140294   2.017390
## B_true[18]   0.03572392 0.015810871  2.006999e-02 0.05664883   2.077705
## B_true[19]   0.03352571 0.017875352  1.530678e-02 0.05636376   2.055232
## B_true[20]   0.13666656 0.085863192  4.020222e-02 0.22531680   2.012307
## B_true[21]   0.10237899 0.051522462  4.446175e-02 0.15469696   2.011914
## B_true[22]   0.09417056 0.043534254  4.492279e-02 0.14001760   2.021258
## B_true[23]   0.03710034 0.014356211  2.277423e-02 0.05641495   2.084131
## B_true[24]   0.03368082 0.017770770  1.604022e-02 0.05642928   2.054166
## B_true[25]   0.03299470 0.018386238  1.473689e-02 0.05634115   2.049587
## B_true[26]   0.03293301 0.018517438  1.414326e-02 0.05652921   2.049485
## B_true[27]   0.02983103 0.021530501  8.487801e-03 0.05636823   2.036652
## B_true[28]   0.09188470 0.041083069  4.466640e-02 0.13381056   2.016596
## B_true[29]   0.09301196 0.042182288  4.462348e-02 0.13609718   2.017934
## B_true[30]   0.09976914 0.048785712  4.466537e-02 0.14988888   2.013847
## B_true[31]   0.09724502 0.046162430  4.503556e-02 0.14607600   2.013608
## B_true[32]   0.09281401 0.041703468  4.521937e-02 0.13490190   2.015423
## B_true[33]   0.14474362 0.093753821  3.892472e-02 0.24135841   2.011222
## B_true[34]   0.12701696 0.077270896  3.821320e-02 0.20898575   2.017248
## B_true[35]   0.13256447 0.082047730  3.770500e-02 0.21574335   2.016479
## B_true[36]   0.12106390 0.071118470  3.485019e-02 0.19457388   2.028240
## B_true[37]   0.09612250 0.045296161  4.159589e-02 0.14270116   2.030605
## B_true[38]   0.08538699 0.034489649  4.409861e-02 0.12128842   2.033369
## B_true[39]   0.08443051 0.033594887  4.461677e-02 0.11955539   2.029296
## B_true[40]   0.08148150 0.030792014  4.454011e-02 0.11325349   2.030413
## B_true[41]   0.09193753 0.040863505  4.416042e-02 0.13391616   2.021768
## B_true[42]   0.08861792 0.038449900  4.155446e-02 0.12744854   2.035658
## B_true[43]   0.09295146 0.042080130  4.443791e-02 0.13729944   2.017424
## B_true[44]   0.09386902 0.043271906  4.312075e-02 0.14228006   2.028136
## B_true[45]   0.10034621 0.049710914  4.245133e-02 0.15104262   2.020728
## B_true[46]   0.09760559 0.047675502  3.958345e-02 0.14570023   2.037521
## B_true[47]   0.08897928 0.038227108  4.334626e-02 0.12945440   2.024657
## B_true[48]   0.09354101 0.043506466  3.911534e-02 0.14106963   2.034204
## B_true[49]   0.09727378 0.047015065  4.072488e-02 0.14508827   2.033092
## B_true[50]   0.08129650 0.030829071  4.325372e-02 0.11302719   2.041355
## B_true[51]   0.08678859 0.035796196  4.415053e-02 0.12425239   2.026910
## B_true[52]   0.08887381 0.037947059  4.439973e-02 0.13094563   2.025237
## B_true[53]   0.03160546 0.019777592  1.182642e-02 0.05636707   2.042713
## B_true[54]   0.02823905 0.023102810  5.291843e-03 0.05642886   2.031181
## B_true[55]   0.07551241 0.025247861  4.416031e-02 0.10432588   2.055735
## B_true[56]   0.09191827 0.040957883  4.430771e-02 0.13367564   2.018640
## B_true[57]   0.08797230 0.037217547  4.321981e-02 0.12738326   2.035847
## B_true[58]   0.09067749 0.040303267  4.179115e-02 0.13214683   2.034003
## B_true[59]   0.10397719 0.055291329  3.448077e-02 0.16207649   2.046191
## B_true[60]   0.10035392 0.051296883  3.650348e-02 0.15260475   2.047443
## B_true[61]   0.09922443 0.048551399  3.849775e-02 0.15027765   2.053487
## B_true[62]   0.10187495 0.051289197  3.641921e-02 0.15404514   2.059712
## B_true[63]   0.10020823 0.049470837  3.976818e-02 0.15111269   2.038857
## B_true[64]   0.07092286 0.020132048  4.498333e-02 0.09133536   2.068904
## B_true[65]   0.12637717 0.077179046  3.515688e-02 0.21273862   2.028642
## B_true[66]   0.04635488 0.005809237  4.064579e-02 0.05662861   2.811974
## B_true[67]   0.04740339 0.005308457  4.261813e-02 0.05723847   3.615721
## B_true[68]   0.04808120 0.004605676  4.427534e-02 0.05674971   4.335641
## B_true[69]   0.05200965 0.003663830  4.554765e-02 0.05702077  22.438862
## B_true[70]   0.05118520 0.003591542  4.537083e-02 0.05693168  27.717120
## B_true[71]   0.05060991 0.003836441  4.538949e-02 0.05719437  27.780936
## B_true[72]   0.04607722 0.006080856  4.034922e-02 0.05659476   2.692194
## B_true[73]   0.05233630 0.003700640  4.575541e-02 0.05689322  15.577269
## B_true[74]   0.03116281 0.020223087  1.092413e-02 0.05635738   2.041019
## B_true[75]   0.03123294 0.020158433  1.117046e-02 0.05646893   2.041611
## B_true[76]   0.03022227 0.021170205  8.995243e-03 0.05642575   2.037784
## B_true[77]   0.02932565 0.022006029  7.353880e-03 0.05638325   2.034473
## B_true[78]   0.02962971 0.021722817  8.060475e-03 0.05638668   2.036007
## B_true[79]   0.02833078 0.023030954  5.393765e-03 0.05635452   2.031181
## B_true[80]   0.02910968 0.022235047  6.940864e-03 0.05627592   2.033308
## B_true[81]   0.55355081 0.457443743  3.930174e-02 1.02354961   2.040217
## B_true[82]   0.55082282 0.457150949  3.669099e-02 1.02604496   2.032355
## B_true[83]   0.03990695 0.011672238  2.833958e-02 0.05657115   2.135869
## B_true[84]   0.03968761 0.011894000  2.809082e-02 0.05668480   2.144257
## B_true[85]   0.05327061 0.004274409  4.539442e-02 0.05698238   7.272029
## B_true[86]   0.11904517 0.068123999  4.144286e-02 0.19005600   2.016243
## B_true[87]   0.11431532 0.063398495  4.146114e-02 0.17910679   2.018017
## B_true[88]   0.12789355 0.076938693  4.160417e-02 0.20736395   2.011107
## B_true[89]   0.11125356 0.061087218  4.054442e-02 0.17502556   2.020895
## B_true[90]   0.11097174 0.060574084  4.092012e-02 0.17239465   2.018240
## B_true[91]   0.06065462 0.011809815  4.036604e-02 0.07087470   2.884819
## B_true[92]   0.12375990 0.073505503  3.949034e-02 0.19971596   2.016684
## B_true[93]   0.04817075 0.004628745  4.306004e-02 0.05643978   4.900633
## B_true[94]   0.03755654 0.013961833  2.350022e-02 0.05638183   2.090467
## B_true[95]   0.03854888 0.012928843  2.576626e-02 0.05637933   2.106464
## B_true[96]   0.03242940 0.018952986  1.362285e-02 0.05646649   2.050421
## B_true[97]   0.03300023 0.018472288  1.475429e-02 0.05660523   2.054081
## B_true[98]   0.03264036 0.018724801  1.398360e-02 0.05637115   2.048372
## B_true[99]   0.03563645 0.015867214  1.984542e-02 0.05652879   2.075324
## B_true[100]  0.03515645 0.016255751  1.866055e-02 0.05647472   2.067980
## B_true[101]  0.03389214 0.017556742  1.614458e-02 0.05654449   2.058828
## B_true[102]  0.12008038 0.071144049  3.715748e-02 0.19346642   2.018576
## B_true[103]  0.13161294 0.081205886  3.888477e-02 0.22234444   2.017099
## B_true[104]  0.03206591 0.019333743  1.290308e-02 0.05640057   2.044925
## B_true[105]  0.03151788 0.019860173  1.168994e-02 0.05637212   2.042840
## B_true[106]  0.12925448 0.078046918  3.506072e-02 0.20983941   2.031072
## B_true[107]  0.11548087 0.066523261  3.276056e-02 0.18414712   2.035962
## B_true[108]  0.10801951 0.057345462  4.207442e-02 0.16697189   2.017246
## B_true[109]  0.08996198 0.039466808  4.326586e-02 0.13067553   2.030805
## B_true[110]  0.13203691 0.082400260  3.655308e-02 0.22316140   2.022866
## B_true[111]  0.11394981 0.063107030  4.137588e-02 0.18216478   2.017371
## B_true[112]  0.11544972 0.065118624  3.977257e-02 0.18143054   2.017570
## B_true[113]  0.13212860 0.083774427  3.330901e-02 0.21718957   2.018614
## B_true[114]  0.12308443 0.072223000  3.879984e-02 0.19811004   2.021755
## B_true[115]  0.10149486 0.051030218  4.233273e-02 0.15470650   2.026816
## B_true[116]  0.11317593 0.063509736  3.834138e-02 0.18263884   2.025996
## B_true[117]  0.09658542 0.045421744  4.224648e-02 0.14282456   2.026894
## B_true[118]  0.11915586 0.069793740  3.267939e-02 0.18972689   2.044876
## B_true[119]  0.17722635 0.126162922  3.166840e-02 0.31011362   2.017438
## B_true[120]  0.18441863 0.127748347  3.210949e-02 0.31331186   2.037854
## B_true[121]  0.02722733 0.024084412  3.316125e-03 0.05631555   2.028689
## B_true[122]  0.02734562 0.023987136  3.499947e-03 0.05634295   2.028799
## B_true[123]  0.03159980 0.019775542  1.180694e-02 0.05647339   2.043676
## B_true[124]  0.11838313 0.069176554  2.902723e-02 0.18856354   2.062104
## B_true[125]  0.14647070 0.096414343  3.496499e-02 0.24342418   2.012971
## B_true[126]  0.03946615 0.012092904  2.745881e-02 0.05646414   2.133318
## B_true[127]  0.03594738 0.015540080  2.054402e-02 0.05646026   2.073966
## B_true[128]  0.03539893 0.016095870  1.942898e-02 0.05644325   2.068003
## B_true[129]  0.03288168 0.018498213  1.441815e-02 0.05639200   2.048798
## B_true[130]  0.03755416 0.014001762  2.372563e-02 0.05649235   2.095304
## B_true[131]  0.03728998 0.014214338  2.300517e-02 0.05644532   2.088479
## B_true[132]  0.37335360 0.312865884  2.462412e-02 0.69478265   2.012682
## B_true[133]  0.43989696 0.375014926  3.039730e-02 0.84618661   2.008345
## B_true[134]  0.40772065 0.332590763  3.479775e-02 0.75038872   2.019088
## B_true[135]  0.41764631 0.350837227  3.131885e-02 0.78640657   2.011702
## B_true[136]  0.19723960 0.145673546  2.576918e-02 0.34544056   2.020629
## B_true[137]  0.19076831 0.139413956  2.856972e-02 0.33226881   2.017526
## B_true[138]  0.17398953 0.123941379  2.880601e-02 0.29994025   2.019325
## B_true[139]  0.17147316 0.120440153  2.940326e-02 0.29302614   2.022931
## B_true[140]  0.20878902 0.153783495  3.073567e-02 0.36732658   2.019022
## B_true[141]  0.03817911 0.013277267  2.479613e-02 0.05636926   2.104848
## B_true[142]  0.03237248 0.019017092  1.347545e-02 0.05642143   2.047014
## B_true[143]  0.08980682 0.039732182  3.751034e-02 0.12987003   2.074199
## B_true[144]  0.08354320 0.033454267  4.139324e-02 0.11765674   2.054013
## B_true[145]  0.09908994 0.048213563  3.854435e-02 0.15137578   2.052015
## B_true[146]  0.05837486 0.008218041  4.566042e-02 0.06683204   2.399979
## B_true[147]  0.43969189 0.357418279  3.609329e-02 0.80122455   2.026938
## B_true[148]  0.41427905 0.342805899  3.098053e-02 0.76820074   2.014759
## B_true[149]  0.10842249 0.056963612  4.159347e-02 0.16960981   2.021533
## B_true[150]  0.09137449 0.040864948  4.089940e-02 0.13288938   2.039180
## B_true[151]  0.07950966 0.029278209  4.225574e-02 0.11079303   2.057818
## B_true[152]  0.05685736 0.007337726  4.412949e-02 0.06481118   3.269095
## B_true[153]  0.05585050 0.006697359  4.354195e-02 0.06139838   3.523992
## B_true[154]  0.06637277 0.016275613  4.119130e-02 0.08241009   2.288898
## B_true[155]  0.06598052 0.015945418  4.152273e-02 0.08370193   2.295852
## B_true[156]  0.05252431 0.004819349  4.361300e-02 0.05776357  13.834327
## B_true[157]  0.11698172 0.068507098  3.298823e-02 0.18609713   2.029846
## B_true[158]  0.14444691 0.092707185  3.115755e-02 0.23971409   2.036256
## B_true[159]  0.03356869 0.017847169  1.541990e-02 0.05639141   2.053891
## B_true[160]  0.03568152 0.015810449  1.949730e-02 0.05652583   2.069270
## B_true[161]  0.03553550 0.015942861  1.974198e-02 0.05640554   2.067537
## B_true[162]  0.03542552 0.015989782  1.942221e-02 0.05640899   2.068108
## B_true[163]  0.03683319 0.014606374  2.241518e-02 0.05650680   2.082485
## B_true[164]  0.03535059 0.016122403  1.920147e-02 0.05646291   2.066573
## B_true[165]  0.03547781 0.015963713  1.953861e-02 0.05644872   2.067977
## B_true[166]  0.05119699 0.003123965  4.623772e-02 0.05645914  21.735361
## B_true[167]  0.05061782 0.003310397  4.601547e-02 0.05641850  17.608749
## B_true[168]  0.13840464 0.087117841  3.106359e-02 0.22641036   2.036044
## B_true[169]  0.14987381 0.099002624  3.488493e-02 0.25140609   2.018419
## B_true[170]  0.02881408 0.022539778  6.431969e-03 0.05637644   2.033435
## B_true[171]  0.02858468 0.022756599  5.891930e-03 0.05642484   2.031971
## B_true[172]  0.02901811 0.022334443  6.838126e-03 0.05636983   2.033392
## B_true[173]  0.16177734 0.109900759  3.367491e-02 0.27249528   2.023536
## B_true[174]  0.08461220 0.034117622  4.141142e-02 0.11910663   2.060764
## B_true[175]  0.10840164 0.057489968  3.582117e-02 0.16576545   2.050751
## B_true[176]  0.11353367 0.062435749  3.502563e-02 0.17997816   2.048491
## B_true[177]  0.08833981 0.038815751  3.795329e-02 0.12727674   2.069530
## B_true[178]  0.10020403 0.050253355  3.812716e-02 0.15320181   2.043638
## B_true[179]  0.13183301 0.079995926  3.418610e-02 0.21284158   2.032745
## B_true[180]  0.08708917 0.037110026  3.989992e-02 0.12502800   2.054772
## B_true[181]  0.05739056 0.007510799  4.469065e-02 0.06438893   3.046743
## B_true[182]  0.05872526 0.008639722  4.446381e-02 0.06790966   2.578741
## M_obs       -0.08708053 0.497131332 -1.367808e+00 0.19137530   4.505746
## M_true[1]    0.09920437 0.075452985  1.561996e-02 0.21375676   2.237439
## M_true[2]    0.10910197 0.070269678  2.783134e-02 0.22681618   2.425913
## M_true[3]    0.10455842 0.077491679  1.940726e-02 0.22181500   2.305528
## M_true[4]    0.10305021 0.074668243  9.818243e-03 0.21611906   2.257445
## M_true[5]    0.11291695 0.063810256  4.785940e-02 0.22224693   2.550727
## M_true[6]    0.12752975 0.072624910  6.415308e-02 0.26821084   3.044859
## M_true[7]    0.10650383 0.072631057  2.858718e-02 0.23109783   2.458574
## M_true[8]    0.10786537 0.071481728  3.071739e-02 0.22303701   2.383888
## M_true[9]    0.09298302 0.080679164  9.061821e-03 0.19450142   2.063628
## M_true[10]   0.09259457 0.080757491  5.549216e-03 0.19268508   2.058323
## M_true[11]   0.09326653 0.080281751  7.629054e-03 0.19394469   2.066013
## M_true[12]   0.09158214 0.082128019  7.114866e-04 0.19145110   2.052912
## M_true[13]   0.09178869 0.081841236  1.972673e-03 0.19155876   2.053044
## M_true[14]   0.11902075 0.068932425  4.959088e-02 0.24306121   3.166659
## M_true[15]   0.11509345 0.070378708  3.862405e-02 0.24516384   3.214141
## M_true[16]   0.11991852 0.064174018  5.779948e-02 0.24249991   3.139073
## M_true[17]   0.11415533 0.069545881  3.926695e-02 0.23734896   2.816249
## M_true[18]   0.09363554 0.080268910  9.101052e-03 0.19401898   2.061348
## M_true[19]   0.09285868 0.080868006  5.764201e-03 0.19290313   2.059034
## M_true[20]   0.10970434 0.071456381  3.482396e-02 0.23549329   2.527472
## M_true[21]   0.09686219 0.077979154  1.229816e-02 0.20364077   2.113942
## M_true[22]   0.09838761 0.075225556  1.935476e-02 0.20248842   2.121722
## M_true[23]   0.09234428 0.081278654  3.503360e-03 0.19220711   2.055455
## M_true[24]   0.09204749 0.081767185  2.202702e-03 0.19176866   2.054115
## M_true[25]   0.09183206 0.081847524  2.096452e-03 0.19171278   2.053532
## M_true[26]   0.09185984 0.081934049  2.070117e-03 0.19176520   2.053168
## M_true[27]   0.09141887 0.082304073  5.916354e-07 0.19139065   2.052204
## M_true[28]   0.09588317 0.077867892  1.183935e-02 0.19991685   2.100398
## M_true[29]   0.10146510 0.073474534  2.224466e-02 0.20408005   2.139447
## M_true[30]   0.10234723 0.073195213  2.189081e-02 0.20240357   2.127801
## M_true[31]   0.09970227 0.075605692  1.928625e-02 0.20475499   2.140041
## M_true[32]   0.09686720 0.077876562  1.560857e-02 0.20045122   2.096782
## M_true[33]   0.11696512 0.067890020  5.677002e-02 0.24119190   2.811479
## M_true[34]   0.10598927 0.070402964  2.002535e-02 0.22724864   2.592329
## M_true[35]   0.11242036 0.071721966  3.330800e-02 0.23805592   2.625335
## M_true[36]   0.11766487 0.066700817  5.503193e-02 0.23706460   3.013719
## M_true[37]   0.10690057 0.072501419  3.284009e-02 0.22266927   2.313006
## M_true[38]   0.10577366 0.071699024  2.833001e-02 0.20935726   2.183140
## M_true[39]   0.09906300 0.076305998  1.900273e-02 0.20702040   2.139538
## M_true[40]   0.10119495 0.073521174  2.440035e-02 0.20186448   2.123914
## M_true[41]   0.10437418 0.072624663  2.753749e-02 0.20857124   2.164869
## M_true[42]   0.10464485 0.069805122  3.461900e-02 0.21077724   2.251962
## M_true[43]   0.09864936 0.075720187  2.189022e-02 0.20334658   2.114140
## M_true[44]   0.10012352 0.075701177  1.686385e-02 0.21210946   2.203300
## M_true[45]   0.10491495 0.070722712  3.436682e-02 0.21158110   2.246845
## M_true[46]   0.10703929 0.070793859  3.257685e-02 0.22340483   2.426262
## M_true[47]   0.09816410 0.077323552  1.407615e-02 0.20882190   2.150472
## M_true[48]   0.10466674 0.071209147  2.836468e-02 0.21992639   2.381232
## M_true[49]   0.10503599 0.072003905  2.547337e-02 0.21767163   2.301878
## M_true[50]   0.09747265 0.076165574  1.433006e-02 0.20453892   2.148367
## M_true[51]   0.10332154 0.073507862  2.067562e-02 0.20622544   2.182211
## M_true[52]   0.09845922 0.077945839  1.120744e-02 0.20597590   2.116595
## M_true[53]   0.09216264 0.081610666  2.931102e-03 0.19187594   2.053506
## M_true[54]   0.09142239 0.082303386  7.764085e-07 0.19135333   2.052369
## M_true[55]   0.10576178 0.069102487  2.311617e-02 0.20156532   2.174960
## M_true[56]   0.10328898 0.073076897  2.630736e-02 0.20937407   2.173750
## M_true[57]   0.10013317 0.077363267  1.492130e-02 0.21476093   2.199336
## M_true[58]   0.09981941 0.076442672  1.560544e-02 0.21260058   2.193060
## M_true[59]   0.12134838 0.063036421  6.187174e-02 0.23866261   3.702524
## M_true[60]   0.11794271 0.063440872  6.126584e-02 0.23632394   3.356324
## M_true[61]   0.11967382 0.070997318  4.486069e-02 0.25029201   3.034287
## M_true[62]   0.12134736 0.068575517  5.400752e-02 0.25110938   3.084369
## M_true[63]   0.11774302 0.066503861  5.645060e-02 0.23556431   2.805096
## M_true[64]   0.09702374 0.078174401  9.116865e-03 0.20137685   2.113301
## M_true[65]   0.12643475 0.063493179  7.241743e-02 0.24735474   3.800672
## M_true[66]   0.09382723 0.080206264  8.611487e-03 0.19409759   2.063828
## M_true[67]   0.10101240 0.074591103  1.789349e-02 0.20010690   2.102574
## M_true[68]   0.09744092 0.077521612  1.553487e-02 0.19882950   2.095949
## M_true[69]   0.09816549 0.076678848  1.707845e-02 0.20062881   2.106375
## M_true[70]   0.09467123 0.079476705  1.094046e-02 0.19886872   2.082442
## M_true[71]   0.09870668 0.076550470  1.818837e-02 0.20101373   2.096443
## M_true[72]   0.09353137 0.080675014  9.242452e-03 0.19453915   2.062453
## M_true[73]   0.09586421 0.078097172  1.490774e-02 0.19855660   2.080989
## M_true[74]   0.09174179 0.081925101  1.428672e-03 0.19179868   2.053224
## M_true[75]   0.09174617 0.081993730  1.243171e-03 0.19163768   2.052938
## M_true[76]   0.09154970 0.082175023  6.474208e-04 0.19164502   2.052919
## M_true[77]   0.09140442 0.082273463  8.357250e-07 0.19143688   2.052638
## M_true[78]   0.09153952 0.082187775  4.438224e-04 0.19155506   2.052601
## M_true[79]   0.09142765 0.082271333  2.532142e-07 0.19137851   2.052333
## M_true[80]   0.09140800 0.082292816  7.171255e-07 0.19145791   2.052355
## M_true[81]   0.74106526 0.356792934  9.439065e-02 1.00728978   4.496949
## M_true[82]   0.70695178 0.334638848  9.552366e-02 0.96002214   4.705803
## M_true[83]   0.09302333 0.080531432  5.299516e-03 0.19267767   2.057196
## M_true[84]   0.09362999 0.079675125  9.380116e-03 0.19419255   2.067726
## M_true[85]   0.09644396 0.077487914  1.568187e-02 0.19888237   2.092222
## M_true[86]   0.10497302 0.074967137  1.984082e-02 0.21968752   2.311732
## M_true[87]   0.10890766 0.069267845  4.149941e-02 0.22203214   2.375236
## M_true[88]   0.10856992 0.071075092  3.683543e-02 0.21959339   2.316168
## M_true[89]   0.11086347 0.067061882  4.345229e-02 0.22327777   2.478830
## M_true[90]   0.10833859 0.069784613  4.107798e-02 0.21968841   2.349353
## M_true[91]   0.11403797 0.066817758  4.868140e-02 0.23004073   2.539751
## M_true[92]   0.11658353 0.064139509  5.480083e-02 0.22859041   2.839962
## M_true[93]   0.09865390 0.075010530  1.696897e-02 0.19752538   2.090585
## M_true[94]   0.09292415 0.081148796  4.978466e-03 0.19237952   2.056724
## M_true[95]   0.09234897 0.081286238  4.286146e-03 0.19241190   2.056657
## M_true[96]   0.09302795 0.080408160  6.322302e-03 0.19314916   2.059388
## M_true[97]   0.09233553 0.081504651  7.014204e-03 0.19421475   2.060212
## M_true[98]   0.09243138 0.081031535  4.173904e-03 0.19221347   2.056300
## M_true[99]   0.09325043 0.080558202  7.360272e-03 0.19347051   2.062831
## M_true[100]  0.09256047 0.080876850  5.865412e-03 0.19286559   2.059434
## M_true[101]  0.09296745 0.080775624  5.926288e-03 0.19298602   2.059170
## M_true[102]  0.10708166 0.068929239  3.178888e-02 0.22631668   2.596782
## M_true[103]  0.11622475 0.067327762  5.195329e-02 0.23347026   2.678537
## M_true[104]  0.09178376 0.081984471  1.161154e-03 0.19166178   2.052935
## M_true[105]  0.09145087 0.082311452  2.534058e-04 0.19162471   2.052538
## M_true[106]  0.12530354 0.068913612  5.969655e-02 0.25933943   3.842033
## M_true[107]  0.12247060 0.064478359  6.955886e-02 0.24701504   3.800143
## M_true[108]  0.10421101 0.073803630  2.199335e-02 0.22029594   2.301355
## M_true[109]  0.10381442 0.072171199  2.924754e-02 0.21170619   2.225144
## M_true[110]  0.12135936 0.065681144  5.810763e-02 0.24496073   3.382058
## M_true[111]  0.10342215 0.075759586  1.347661e-02 0.22200836   2.332587
## M_true[112]  0.11431688 0.066295085  4.219439e-02 0.22608088   2.587271
## M_true[113]  0.11877927 0.061671713  6.164521e-02 0.23354576   3.536631
## M_true[114]  0.12413614 0.064323290  5.877249e-02 0.23617488   3.103632
## M_true[115]  0.10973404 0.066892372  3.911825e-02 0.21511274   2.328769
## M_true[116]  0.10790464 0.072394037  1.744075e-02 0.23110049   2.600345
## M_true[117]  0.10201714 0.075591963  2.280468e-02 0.21647193   2.228272
## M_true[118]  0.13144701 0.068556820  6.539122e-02 0.26958470   5.437503
## M_true[119]  0.14283440 0.069021896  5.868620e-02 0.27975739   6.914547
## M_true[120]  0.19345373 0.083383205  7.110837e-02 0.35349278 250.610657
## M_true[121]  0.09142888 0.082285985  1.856821e-07 0.19141001   2.052248
## M_true[122]  0.09144579 0.082271815  8.863693e-08 0.19129046   2.052195
## M_true[123]  0.09180325 0.081815922  1.825507e-03 0.19182066   2.053614
## M_true[124]  0.15020725 0.068058993  6.713650e-02 0.28869245  24.800883
## M_true[125]  0.10581095 0.075492661  1.584580e-02 0.23922613   2.683052
## M_true[126]  0.09285306 0.080561722  7.580041e-03 0.19376930   2.062107
## M_true[127]  0.09262696 0.081181636  4.906251e-03 0.19255290   2.056903
## M_true[128]  0.09249031 0.081453557  4.755775e-03 0.19253767   2.055745
## M_true[129]  0.09181225 0.081867285  1.747785e-03 0.19177780   2.053346
## M_true[130]  0.09422333 0.079910754  8.708501e-03 0.19452809   2.062811
## M_true[131]  0.09289565 0.080735420  5.796248e-03 0.19283094   2.058208
## M_true[132]  0.28480836 0.137326261  5.627193e-02 0.52388969 151.844813
## M_true[133]  0.28586093 0.126184831  6.619515e-02 0.51549462 151.218994
## M_true[134]  0.38265861 0.158423059  8.000919e-02 0.61651758 121.388063
## M_true[135]  0.30493633 0.134092015  7.259442e-02 0.52038700 170.799177
## M_true[136]  0.16813005 0.075296072  6.126120e-02 0.31575857  69.017196
## M_true[137]  0.16225345 0.071931652  6.907018e-02 0.31007634  41.538711
## M_true[138]  0.14530839 0.067431298  7.368183e-02 0.27981922  11.458064
## M_true[139]  0.15376534 0.076822868  6.818213e-02 0.30855873  17.702368
## M_true[140]  0.19746649 0.080738534  7.031985e-02 0.35339425 289.094955
## M_true[141]  0.09282140 0.080649602  6.384998e-03 0.19296274   2.060839
## M_true[142]  0.09191525 0.081778548  2.253758e-03 0.19170116   2.054205
## M_true[143]  0.12012509 0.067561517  4.300195e-02 0.24261692   3.061057
## M_true[144]  0.11108495 0.067788765  3.842459e-02 0.21959477   2.465225
## M_true[145]  0.12369176 0.066837606  5.880342e-02 0.24549577   3.041918
## M_true[146]  0.09399451 0.079031464  1.348707e-02 0.19648749   2.075018
## M_true[147]  0.42166453 0.183858435  8.710757e-02 0.71505194 189.278300
## M_true[148]  0.39985009 0.181280958  7.486304e-02 0.66926716 144.896463
## M_true[149]  0.11805708 0.067686619  4.786040e-02 0.23141845   2.596335
## M_true[150]  0.10663897 0.071254116  2.196819e-02 0.22256889   2.400202
## M_true[151]  0.10373236 0.071031187  3.033702e-02 0.21050205   2.198965
## M_true[152]  0.10584346 0.071888435  2.646939e-02 0.20782463   2.202805
## M_true[153]  0.10112077 0.072401677  2.704986e-02 0.20527969   2.172194
## M_true[154]  0.10390073 0.075761534  1.444683e-02 0.22207508   2.357014
## M_true[155]  0.10372505 0.078159061  8.221003e-03 0.23082957   2.348760
## M_true[156]  0.09634988 0.077124128  1.394574e-02 0.20302325   2.144906
## M_true[157]  0.12351694 0.061906712  6.770508e-02 0.24247940   3.778921
## M_true[158]  0.13351296 0.082610587  3.267862e-02 0.29228266   4.698924
## M_true[159]  0.09213024 0.081542504  2.737998e-03 0.19196155   2.054021
## M_true[160]  0.09237520 0.081270747  3.875889e-03 0.19205917   2.055027
## M_true[161]  0.09239853 0.081440175  3.908794e-03 0.19228688   2.055140
## M_true[162]  0.09208372 0.081474714  4.074276e-03 0.19227947   2.055165
## M_true[163]  0.09216983 0.081243692  4.294031e-03 0.19215516   2.056396
## M_true[164]  0.09202154 0.081732227  2.560947e-03 0.19188463   2.054513
## M_true[165]  0.09209782 0.081537588  3.145402e-03 0.19191432   2.054426
## M_true[166]  0.09231914 0.081321324  6.023678e-03 0.19299197   2.058619
## M_true[167]  0.09325904 0.080239252  5.858865e-03 0.19327977   2.058164
## M_true[168]  0.15740412 0.066811671  8.010795e-02 0.28893506  14.756820
## M_true[169]  0.12361642 0.075989396  3.127331e-02 0.25964741   3.284832
## M_true[170]  0.09139303 0.082317274  7.102385e-07 0.19146468   2.052231
## M_true[171]  0.09141104 0.082315538  7.021642e-07 0.19146690   2.052194
## M_true[172]  0.09140292 0.082315655  5.175369e-07 0.19148862   2.052331
## M_true[173]  0.14548158 0.072110098  7.345162e-02 0.29214702   6.897263
## M_true[174]  0.11622153 0.064417495  4.923551e-02 0.22532153   2.534313
## M_true[175]  0.13641090 0.063913822  7.782903e-02 0.25871776   4.551553
## M_true[176]  0.13448113 0.069052716  7.153806e-02 0.27018207   4.155323
## M_true[177]  0.10432885 0.075102952  1.344911e-02 0.22817916   2.462845
## M_true[178]  0.11033392 0.074506877  1.407202e-02 0.24350483   2.710122
## M_true[179]  0.13639385 0.073490949  6.389785e-02 0.27719400   4.105570
## M_true[180]  0.11908437 0.062764493  4.790767e-02 0.22509656   2.687702
## M_true[181]  0.10317341 0.073362814  2.667467e-02 0.20722046   2.155328
## M_true[182]  0.09726132 0.078157122  1.007617e-02 0.20775467   2.129123
## a           -1.11152243 1.115188490 -2.518862e+00 0.64125998   2.215313
## b            0.76499661 0.343319978  2.341609e-01 1.19485252   3.439967
## sigma        1.27895991 1.537166826  5.718494e-03 3.78078104   2.000446
##                  Rhat4
## B_obs        53.720822
## B_true[1]     7.396049
## B_true[2]     5.098279
## B_true[3]     5.265602
## B_true[4]     5.702166
## B_true[5]     4.497430
## B_true[6]     3.407201
## B_true[7]     5.534291
## B_true[8]     6.178504
## B_true[9]     1.940218
## B_true[10]    3.757868
## B_true[11]    3.368301
## B_true[12]    8.077239
## B_true[13]    7.614022
## B_true[14]   11.144179
## B_true[15]   11.563008
## B_true[16]   11.258088
## B_true[17]   11.385857
## B_true[18]    5.798441
## B_true[19]    7.054505
## B_true[20]   13.651092
## B_true[21]   14.193872
## B_true[22]   11.979480
## B_true[23]    5.784149
## B_true[24]    7.129661
## B_true[25]    7.473787
## B_true[26]    7.458910
## B_true[27]    8.689889
## B_true[28]   12.234769
## B_true[29]   11.919638
## B_true[30]   13.029694
## B_true[31]   13.092578
## B_true[32]   12.634864
## B_true[33]   14.064161
## B_true[34]   11.817970
## B_true[35]   11.446067
## B_true[36]    9.302574
## B_true[37]    8.465115
## B_true[38]    8.575012
## B_true[39]    9.004274
## B_true[40]    8.608995
## B_true[41]   10.446051
## B_true[42]    8.200967
## B_true[43]   11.444768
## B_true[44]    9.202288
## B_true[45]   10.483198
## B_true[46]    7.795817
## B_true[47]   10.032110
## B_true[48]    8.285822
## B_true[49]    8.294845
## B_true[50]    7.724647
## B_true[51]    9.256978
## B_true[52]   10.138683
## B_true[53]    8.020890
## B_true[54]    9.443177
## B_true[55]    6.792948
## B_true[56]   11.038827
## B_true[57]    8.029328
## B_true[58]    8.288199
## B_true[59]    7.116936
## B_true[60]    6.747028
## B_true[61]    6.395887
## B_true[62]    6.173513
## B_true[63]    7.520495
## B_true[64]    5.977789
## B_true[65]    9.062737
## B_true[66]    2.144156
## B_true[67]    1.665888
## B_true[68]    1.531217
## B_true[69]    1.176849
## B_true[70]    1.126166
## B_true[71]    1.129457
## B_true[72]    2.236918
## B_true[73]    1.211074
## B_true[74]    8.208506
## B_true[75]    8.132377
## B_true[76]    8.539952
## B_true[77]    8.981624
## B_true[78]    8.744317
## B_true[79]    9.426285
## B_true[80]    9.141532
## B_true[81]    7.488742
## B_true[82]    8.160422
## B_true[83]    4.611611
## B_true[84]    4.366702
## B_true[85]    1.346399
## B_true[86]   11.768016
## B_true[87]   11.252341
## B_true[88]   14.300272
## B_true[89]   11.063066
## B_true[90]   10.918470
## B_true[91]    1.853222
## B_true[92]   11.485719
## B_true[93]    1.477673
## B_true[94]    5.506390
## B_true[95]    5.145224
## B_true[96]    7.365055
## B_true[97]    7.092280
## B_true[98]    7.545921
## B_true[99]    5.982697
## B_true[100]   6.355433
## B_true[101]   6.808584
## B_true[102]  11.244327
## B_true[103]  11.437438
## B_true[104]   7.850183
## B_true[105]   8.008799
## B_true[106]   8.361665
## B_true[107]   7.910710
## B_true[108]  11.309893
## B_true[109]   8.866830
## B_true[110]   9.953171
## B_true[111]  11.857238
## B_true[112]  11.368142
## B_true[113]  10.804931
## B_true[114]  10.171002
## B_true[115]   9.664343
## B_true[116]   9.392679
## B_true[117]   9.564148
## B_true[118]   6.983281
## B_true[119]  11.340812
## B_true[120]   7.554533
## B_true[121]   9.849752
## B_true[122]   9.823173
## B_true[123]   7.962020
## B_true[124]   5.941045
## B_true[125]  13.151924
## B_true[126]   4.575207
## B_true[127]   6.139039
## B_true[128]   6.331154
## B_true[129]   7.492724
## B_true[130]   5.283366
## B_true[131]   5.594613
## B_true[132]  13.328910
## B_true[133]  16.486260
## B_true[134]  10.888301
## B_true[135]  13.599196
## B_true[136]  10.184435
## B_true[137]  11.038590
## B_true[138]  10.524917
## B_true[139]   9.418148
## B_true[140]  10.500827
## B_true[141]   5.147320
## B_true[142]   7.680636
## B_true[143]   5.378418
## B_true[144]   6.374230
## B_true[145]   6.563795
## B_true[146]   2.771036
## B_true[147]   8.905699
## B_true[148]  12.497085
## B_true[149]  10.029545
## B_true[150]   7.324360
## B_true[151]   6.224406
## B_true[152]   1.748373
## B_true[153]   1.648391
## B_true[154]   2.891930
## B_true[155]   3.002051
## B_true[156]   1.230910
## B_true[157]   8.396424
## B_true[158]   7.866204
## B_true[159]   7.182349
## B_true[160]   6.326189
## B_true[161]   6.375518
## B_true[162]   6.380129
## B_true[163]   5.822735
## B_true[164]   6.456162
## B_true[165]   6.376190
## B_true[166]   1.175425
## B_true[167]   1.224870
## B_true[168]   7.695502
## B_true[169]  10.688238
## B_true[170]   9.103467
## B_true[171]   9.298350
## B_true[172]   9.067312
## B_true[173]   9.361430
## B_true[174]   5.983210
## B_true[175]   6.638544
## B_true[176]   6.656230
## B_true[177]   5.599336
## B_true[178]   7.126534
## B_true[179]   7.983689
## B_true[180]   6.341803
## B_true[181]   1.868261
## B_true[182]   2.275499
## M_obs         4.923829
## M_true[1]     3.166330
## M_true[2]     2.511828
## M_true[3]     2.882053
## M_true[4]     3.171950
## M_true[5]     2.205027
## M_true[6]     1.812826
## M_true[7]     2.377946
## M_true[8]     2.547588
## M_true[9]     6.382357
## M_true[10]    6.764784
## M_true[11]    6.348189
## M_true[12]    7.184600
## M_true[13]    7.143313
## M_true[14]    1.735401
## M_true[15]    1.730331
## M_true[16]    1.733383
## M_true[17]    1.933073
## M_true[18]    6.425553
## M_true[19]    6.791592
## M_true[20]    2.261348
## M_true[21]    4.471473
## M_true[22]    4.434643
## M_true[23]    6.981884
## M_true[24]    7.104739
## M_true[25]    7.121899
## M_true[26]    7.152341
## M_true[27]    7.238268
## M_true[28]    4.829244
## M_true[29]    4.173741
## M_true[30]    4.296376
## M_true[31]    4.171111
## M_true[32]    4.991967
## M_true[33]    1.947776
## M_true[34]    2.185540
## M_true[35]    2.120824
## M_true[36]    1.807374
## M_true[37]    2.798856
## M_true[38]    3.644868
## M_true[39]    4.098498
## M_true[40]    4.336293
## M_true[41]    3.734751
## M_true[42]    3.108222
## M_true[43]    4.441879
## M_true[44]    3.430636
## M_true[45]    3.119908
## M_true[46]    2.462810
## M_true[47]    3.875054
## M_true[48]    2.603748
## M_true[49]    2.823552
## M_true[50]    3.996840
## M_true[51]    4.006786
## M_true[52]    4.350788
## M_true[53]    7.110316
## M_true[54]    7.227163
## M_true[55]    3.792870
## M_true[56]    3.738050
## M_true[57]    3.512320
## M_true[58]    3.474266
## M_true[59]    1.560397
## M_true[60]    1.658802
## M_true[61]    1.800205
## M_true[62]    1.748921
## M_true[63]    1.919514
## M_true[64]    4.863216
## M_true[65]    1.518702
## M_true[66]    6.380668
## M_true[67]    4.781399
## M_true[68]    5.011175
## M_true[69]    4.835610
## M_true[70]    5.301845
## M_true[71]    4.986706
## M_true[72]    6.341804
## M_true[73]    5.486614
## M_true[74]    7.142030
## M_true[75]    7.183498
## M_true[76]    7.185672
## M_true[77]    7.202586
## M_true[78]    7.201150
## M_true[79]    7.230605
## M_true[80]    7.219242
## M_true[81]    1.423081
## M_true[82]    1.394760
## M_true[83]    6.789001
## M_true[84]    6.150749
## M_true[85]    5.168474
## M_true[86]    2.806056
## M_true[87]    2.604691
## M_true[88]    2.754291
## M_true[89]    2.389112
## M_true[90]    2.638355
## M_true[91]    2.287439
## M_true[92]    1.954061
## M_true[93]    5.167720
## M_true[94]    6.882309
## M_true[95]    6.943250
## M_true[96]    6.655393
## M_true[97]    6.571926
## M_true[98]    6.869126
## M_true[99]    6.508583
## M_true[100]   6.681870
## M_true[101]   6.709724
## M_true[102]   2.155314
## M_true[103]   2.048336
## M_true[104]   7.176685
## M_true[105]   7.221525
## M_true[106]   1.523872
## M_true[107]   1.514426
## M_true[108]   2.829724
## M_true[109]   3.256686
## M_true[110]   1.602000
## M_true[111]   2.796410
## M_true[112]   2.176577
## M_true[113]   1.577079
## M_true[114]   1.817780
## M_true[115]   2.776699
## M_true[116]   2.167794
## M_true[117]   3.247736
## M_true[118]   1.329817
## M_true[119]   1.276189
## M_true[120]   1.019480
## M_true[121]   7.235656
## M_true[122]   7.239339
## M_true[123]   7.116079
## M_true[124]   1.129412
## M_true[125]   2.035914
## M_true[126]   6.484846
## M_true[127]   6.870633
## M_true[128]   6.929357
## M_true[129]   7.129276
## M_true[130]   6.388775
## M_true[131]   6.727365
## M_true[132]   1.044486
## M_true[133]   1.015881
## M_true[134]   1.057695
## M_true[135]   1.040512
## M_true[136]   1.063685
## M_true[137]   1.103012
## M_true[138]   1.164669
## M_true[139]   1.121869
## M_true[140]   1.022128
## M_true[141]   6.631248
## M_true[142]   7.103465
## M_true[143]   1.790162
## M_true[144]   2.462580
## M_true[145]   1.819995
## M_true[146]   5.689107
## M_true[147]   1.044891
## M_true[148]   1.077704
## M_true[149]   2.235679
## M_true[150]   2.597581
## M_true[151]   3.380858
## M_true[152]   3.578012
## M_true[153]   3.722922
## M_true[154]   2.664057
## M_true[155]   2.762189
## M_true[156]   4.013906
## M_true[157]   1.501725
## M_true[158]   1.413325
## M_true[159]   7.075913
## M_true[160]   6.974513
## M_true[161]   6.999284
## M_true[162]   6.970015
## M_true[163]   6.907818
## M_true[164]   7.072573
## M_true[165]   7.066910
## M_true[166]   6.809529
## M_true[167]   6.681434
## M_true[168]   1.132682
## M_true[169]   1.650657
## M_true[170]   7.235564
## M_true[171]   7.239771
## M_true[172]   7.231024
## M_true[173]   1.232600
## M_true[174]   2.270574
## M_true[175]   1.429956
## M_true[176]   1.439796
## M_true[177]   2.344219
## M_true[178]   2.063550
## M_true[179]   1.437650
## M_true[180]   2.026680
## M_true[181]   3.981761
## M_true[182]   4.217962
## a             3.431615
## b             1.749514
## sigma       221.257616
```

```r
compare(m1.1,m1.2)
```

```
## Warning in compare(m1.1, m1.2): Different numbers of observations found for at least two models.
## Model comparison is valid only for models fit to exactly the same observations.
## Number of observations for each model:
## m1.1 182 
## m1.2 N
```

```
## Error in check_pars(allpars, pars): no parameter log_lik
```



# 2. Now consider missing values—this data set is lousy with them. You can ignore measurement error in this problem. Let’s get a quick idea of the missing values by counting them in each variable:

```r
library(rethinking)
data(Primates301)
d <- Primates301
colSums( is.na(d) )
```

```
##                name               genus             species          subspecies 
##                   0                   0                   0                 267 
##              spp_id            genus_id     social_learning     research_effort 
##                   0                   0                  98                 115 
##               brain                body          group_size           gestation 
##                 117                  63                 114                 161 
##             weaning           longevity        sex_maturity maternal_investment 
##                 185                 181                 194                 197
```

* We’ll continue to focus on just brain and body, to stave off insanity. Consider only those species with measured body masses:

```r
cc <- complete.cases( d$body )
M <- d$body[cc]
M <- M / max(M)
B <- d$brain[cc]
B <- B / max( B , na.rm=TRUE )
```

* You should end up with 238 species and 56 missing brain values among them. First, consider whether there is a pattern to the missing values. Does it look like missing values are associated with particular values of body mass? Draw a DAG that represents how missingness works in this case. Which type (MCAR, MAR, MNAR)
is this?
Second, impute missing values for brain size. It might help to initialize the 56
imputed variables to a valid value:

```r
 start=list( B_impute=rep(0.5,56) )
```

* This just helps the chain get started.
* Compare the inferences to an analysis that drops all the missing values. Has any-
thing changed? Why or why not? Hint: Consider the density of data in the ranges where there are missing values. You might want to plot the imputed brain sizes to- gether with the observed values.

# optional 15H4

