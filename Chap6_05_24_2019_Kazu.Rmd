---
title: "Chap6_05_24_2019"
author: "Kazu"
date: "5/16/2019"
output: 
  html_document: 
    keep_md: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
#install.packages(c("devtools","mvtnorm","loo","coda"),dependencies=TRUE)
#library(devtools)
#install_github("rmcelreath/rethinking",ref="Experimental")
library(rethinking)
library(lme4);library(lmerTest);library(ggplot2);library(reshape2);library(tidyverse);library(readr)
```

Overthinking: Simulated science distortion. Simulations like this one are easy to do in R, or in any other scripting language, once you have seen a few examples. In this simulation, we just draw some random Gaussian criteria for a sample of proposals and then select the top 10% combined scores.
# R code 6.1
```{r}
set.seed(1914)
N <- 200 # num grant proposals
p <- 0.1 # proportion to select
# uncorrelated newsworthiness and trustworthiness 
nw <- rnorm(N)
tw <- rnorm(N)
# select top 10% of combined scores
s<-nw+tw #totalscore
q <- quantile( s , 1-p ) # top 10% threshold 
selected <- ifelse( s >= q , TRUE , FALSE )
cor( tw[selected] , nw[selected] )
```

# 6.1. Multicollinearity
6.1.1. Multicollinear legs.
The code below will simulate the heights and leg lengths of 100 individuals.
```{r}
N <- 100  # number of individuals
set.seed(909)
height <- rnorm(N,10,2) # sim total height of each
leg_prop <- runif(N,0.4,0.5) # leg as proportion of height
leg_left <- leg_prop*height + rnorm(N,0,0.02) # Sim left leg as proportion + error
leg_right <- leg_prop*height + rnorm(N,0,0.02) # sim right leg as proportion + error
d <- data.frame(height, leg_left,leg_right) # combine into data frame

```
# R code 6.3
```{r}
m6.1 <- quap(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left + br*leg_right ,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
        br ~ dnorm( 2 , 10 ) ,
        sigma ~ dexp( 1 )
),
    data=d )
precis(m6.1)
```

# R code 6.4
```{r}
plot(precis(m6.1))
```

# R code 6.5
```{r}
post <- extract.samples(m6.1)
plot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )
```

# R code 6.6
```{r}
sum_blbr <- post$bl + post$br
dens( sum_blbr , col=rangi2 , lwd=2 , xlab="sum of bl and br")
```

# R code 6.7
```{r}
m6.2 <- quap(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + bl*leg_left,
        a ~ dnorm( 10 , 100 ) ,
        bl ~ dnorm( 2 , 10 ) ,
sigma ~ dexp( 1 ) ),
    data=d )
precis(m6.2)
```
# 6.1.2. Multicollinearmilk.
# R code 6.8
```{r}
library(rethinking)
data(milk)
d <- milk
d$K <- scale( d$kcal.per.g )
d$F <- scale( d$perc.fat )
d$L <- scale( d$perc.lactose )
```

# R code 6.9
```{r}
# kcal.per.g regressed on perc.fat
m6.3 <- quap(
    alist(
        K ~ dnorm( mu , sigma ) ,
        mu <- a + bF*F ,
        a ~ dnorm( 0 , 0.2 ) ,
        bF ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
) , data=d )
# kcal.per.g regressed on perc.lactose
m6.4 <- quap(
    alist(
        K ~ dnorm( mu , sigma ) ,
        mu <- a + bL*L ,
        a ~ dnorm( 0 , 0.2 ) ,
        bL ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
) , data=d )
precis( m6.3 )
precis( m6.4 )
```

# R 6.10
```{r}
m6.5 <- quap(
    alist(
        K ~ dnorm( mu , sigma ) ,
        mu <- a + bF*F + bL*L ,
        a ~ dnorm( 0 , 0.2 ) ,
        bF ~ dnorm( 0 , 0.5 ) ,
        bL ~ dnorm( 0 , 0.5 ) ,
        sigma ~ dexp( 1 )
),
    data=d )
precis( m6.5 )
```

# R code 6.11
```{r}
 pairs( ~ kcal.per.g + perc.fat + perc.lactose , data=d , col=rangi2 )
```

## 6.1.3. Howbadiscorrelation?
# R code 6.12
```{r}
cor( d$perc.fat , d$perc.lactose )
```
# Overthinking: Simulating collinearity. 
```{r}
library(rethinking)
data(milk)
d <- milk
sim.coll <- function( r=0.9 ) {
    d$x <- rnorm( nrow(d) , mean=r*d$perc.fat ,
        sd=sqrt( (1-r^2)*var(d$perc.fat) ) )
    m <- lm( kcal.per.g ~ perc.fat + x , data=d )
    sqrt( diag( vcov(m) ) )[2] # stddev of parameter
}
rep.sim.coll <- function( r=0.9 , n=100 ) {
    stddev <- replicate( n , sim.coll(r) )
    mean(stddev)
}
r.seq <- seq(from=0,to=0.99,by=0.01)
stddev <- sapply( r.seq , function(z) rep.sim.coll(r=z,n=100) )
plot( stddev ~ r.seq , type="l" , col=rangi2, lwd=2 , xlab="correlation" )
```

# 6.2. Post-treatmentbias
## It is routine to worry about mistaken inferences that arise from omitting predictor vari- ables. Such mistakes are often called omitted variable bias, and the examples from the previous chapter illustrate it. It is much less routine to worry about mistaken inferences aris- ing from including variables that are consequences of other variables. We’ll call this post- treatment bias
## Let's simulate some data, to make the example more transparent and see what exactly goes wrong when we include a post-treatment variable.
# R code 6.14
```{r}
set.seed(71)
# number of plants
N <- 100
# simulate initial heights
h0 <- rnorm(N,10,2)
# assign treatments and simulate fungus and growth
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 )
h1 <- h0 + rnorm(N, 5 - 3*fungus)
# compose a clean data frame
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )
 precis(d)
```
# 6.2.1. A prior is born.
# R code 6.15
```{r}
sim_p <- rlnorm( 1e4 , 0 , 0.25 )
precis( data.frame(sim_p) )
```
# R code 6.16
```{r}
m6.6 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0*p,
        p ~ dlnorm( 0 , 0.25 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.6)
```
# R code 6.17
```{r}
m6.7 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment + bf*fungus,
        a ~ dlnorm( 0 , 0.2 ) ,
        bt ~ dnorm( 0 , 0.5 ),
        bf ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.7)
```

# R code 6.18
```{r}
m6.8 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment,
        a ~ dlnorm( 0 , 0.2 ),
        bt ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.8)
```

# 6.2.3. Fungusandd-separation
# R code 6.19
```{r}
library(dagitty)
plant_dag <- dagitty( "dag {
    H0 -> H1
    F -> H1
    T -> F
}")
coordinates( plant_dag ) <- list( x=c(H0=0,T=2,F=1.5,H1=1) ,
y=c(H0=0,T=0,F=1,H1=2) )
plot( plant_dag )
```

# R code 6.20
### I do not understand dseparated()... (see ?dseparated)
```{r}
dseparated( plant_dag , "T" , "H1" )
dseparated( plant_dag , "H1","T" ) # how about reverse oriengation?

dseparated( plant_dag , "T" , "H1" , "F" )
# MO
dseparated( plant_dag , "T" , "F" )

```

# R code 6.21
```{r}
 impliedConditionalIndependencies( plant_dag )
```

# 6.3. Colliderbias
* At the start of the chapter, I argued that all that is necessary for scientific studies to show a negative association between trustworthiness and newsworthiness is that selection processes—grant and journal review—care about both. Now I want to explain how this same selection phenomenon can happen inside a statistical model. When it does, it can seriously distort our inferences, a phenomenon known as collider bias.
## 6.3.1. Collideroffalsesorrow.
# R code 6.22
```{r}
library(rethinking)
d <- sim_happiness( seed=1977 , N_years=1000 )
precis(d)
```
# R code 6.23
```{r}
d2 <- d[ d$age>17 , ] # only adults
d2$A <- ( d2$age - 18 ) / ( 65 - 18 )
```

# R code 6.24
```{r}
d2$mid <- d2$married + 1
m6.9 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a[mid] + bA*A,
        a[mid] ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
precis(m6.9,depth=2)
```

# R code 6.25
```{r}
m6.10 <- quap(
    alist(
        happiness ~ dnorm( mu , sigma ),
        mu <- a + bA*A,
        a ~ dnorm( 0 , 1 ),
        bA ~ dnorm( 0 , 2 ),
        sigma ~ dexp(1)
    ) , data=d2 )
precis(m6.10)
```
# 6.3.2. ThehauntedDAG.
# R code 6.26
```{r}
N <- 200  # number of grandparent-parent-child triads
b_GP <- 1 # direct effect of G on P
b_GC <- 0 # direct effect of G on C
b_PC <- 1 # direct effect of P on C
b_U<-2 #directeffectofUonPandC
```
# R code 6.27
```{r}
set.seed(1)
U <- 2*rbern( N , 0.5 ) - 1 # Random Sample From Bernoulli Distribution 
G <- rnorm( N )
P <- rnorm( N , b_GP*G + b_U*U )
C <- rnorm( N , b_PC*P + b_GC*G + b_U*U )
d <- data.frame( C=C , P=P , G=G , U=U )
```
# R code 6.28
```{r}
m6.11 <- quap(
    alist(
        C ~ dnorm( mu , sigma ),
        mu <- a + b_PC*P + b_GC*G,
        a ~ dnorm( 0 , 1 ),
        c(b_PC,b_GC) ~ dnorm( 0 , 1 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.11)
```

# R code 6.29
```{r}
m6.12 <- quap(
    alist(
        C ~ dnorm( mu , sigma ),
        mu <- a + b_PC*P + b_GC*G + b_U*U,
        a ~ dnorm( 0 , 1 ),
        c(b_PC,b_GC,b_U) ~ dnorm( 0 , 1 ),
        sigma ~ dexp( 1 )
    ), data=d )
precis(m6.12)
```

# 6.4. Confronting confounding (Page 180). 
# R cod 6.30
```{r}
library(dagitty)
dag_6.1 <- dagitty("dag {
            X -> Y <- C
            X <- U -> B
            U <- A -> C
            U -> B <- C
} ")
adjustmentSets(dag_6.1,exposure="X",outcome="Y")
```

# R code 6.31
```{r}
library(dagitty)
dag_6.2 <- dagitty("dag {
                   S -> A -> D
                  S -> M -> D
                  S -> W -> D
                  A -> M
                   }")
adjustmentSets(dag_6.2,exposure="W",outcome="D")
impliedConditionalIndependencies(dag_6.2)
```
* Read the first as “median age of marriage should be independent of (_||_) Waffle Houses, conditioning on (|) a State being in the south.” In the second, divorce and being in the south should be independent when we simultaneously condition on all of median age of marriage, marriage rate, and Waffle Houses. Finally, marriage rate and Waffle Houses should be independent, conditioning on being in the south.(Page 185). 

# problems
Problems: 6H1 6H2 and the three problems at https://github.com/rmcelreath/statrethinking_winter2019/blob/master/homework/week03.pdf
## 6H1. Use the Waffle House data, data(WaffleDivorce), to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph. (Page 186). 
```{r}
data(WaffleDivorce)
d <- WaffleDivorce 
# standardize variables
d$A <- scale( d$MedianAgeMarriage ) # midian scaled age marriage
d$D <- scale( d$Divorce ) # scaled divorce rate
# S: wheather or not a State is the southern Unitead States.
# M: marriage rate
# W: number of waffle house 

library(dagitty) 
dag_6.2 <- dagitty( "dag { S -> A -> D S -> M -> D S -> W -> D A -> M }")
adjustmentSets( dag_6.2 , exposure="W" , outcome="D" )
## how to solve the problem?????

```
## 6H2. Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be amended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren’t in the data. (Page 186). 
```{r}

```

## https://github.com/rmcelreath/statrethinking_winter2019/blob/master/homework/week03.pdf
# homework1:
```{r}
data(foxes)

```

## 

