---
title: "Chap10"
author: "Kazu"
date: "10/18/2019"
output: 
  html_document: 
    keep_md: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,error=TRUE,cache = TRUE)
library(rethinking)
library(lme4);library(lmerTest);library(ggplot2);library(reshape2);library(tidyverse);library(readr)
```

```{r R code 10.1}
p <- list()
p$A <- c(0,0,10,0,0)
p$B <- c(0,1,8,1,0)
p$C <- c(0,2,6,2,0)
p$D <- c(1,2,4,2,1)
p$E <- c(2,2,2,2,2)
```
* normalize each such that it is a probability distribution
```{r R code 10.2}
 p_norm <- lapply( p , function(q) q/sum(q))
```

* Since these are now probability distributions, we can compute the information entropy of each. The only trick here is to remember L’Hôpital’s rule (see page 207):
```{r R code 10.3}
 ( H <- sapply( p_norm , function(q) -sum(ifelse(q==0,0,q*log(q))) ) )
```

* The bottom-right plot in Figure 10.1 displays these logwayspp values against the infor- mation entropies H. These two sets of values contain the same information, as information entropy is an approximation of the log ways per pebble (see the Overthinking box at the end for details).
```{r R code 10.4}
ways <- c(1,90,1260,37800,113400)
logwayspp <- log(ways)/10
```
* This is useful, because the distribution that can happen the greatest number of ways is the most plausible distribution. Call this distribution the maximum entropy distribution.

# 10.1.1 Gaussian

# 10.1.2 Binomial
```{r R code 10.5}
# build list of the candidate distributions
p <- list()
p[[1]] <- c(1/4,1/4,1/4,1/4)
p[[2]] <- c(2/6,1/6,1/6,2/6)
p[[3]] <- c(1/6,2/6,2/6,1/6)
p[[4]] <- c(1/8,4/8,2/8,1/8)
# compute expected value of each
sapply( p , function(p) sum(p*c(0,1,1,2)) )
```

```{r R code 10.6}
# compute entropy of each distribution
sapply( p , function(p) -sum( p*log(p) ) )
```

```{r R code 10.7}
p <- 0.7
( A <- c( (1-p)^2 , p*(1-p) , (1-p)*p , p^2 ) )
```

```{r R code 10.8}
 -sum( A*log(A) )
```

* simulate random probability distributions that have any specified expected value. 
```{r R code 10.9}
sim.p <- function(G=1.4) {
    x123 <- runif(3)
    x4 <- ( (G)*sum(x123)-x123[2]-x123[3] )/(2-G)
    z <- sum( c(x123,x4) )
    p <- c( x123 , x4 )/z
    list( H=-sum( p*log(p) ) , p=p )
}
```

* This function generates a random distribution with expected value G and then returns its entropy along with the distribution. We want to invoke this function a large number of times. Here is how to call it 100000 times and then plot the distribution of resulting entropies:
```{r R code 10.10}
H <- replicate( 1e5 , sim.p(1.4) )
dens( as.numeric(H[1,]) , adj=0.1 )
```

```{r R code 10.11}
entropies <- as.numeric(H[1,])
distributions <- H[2,]
```

```{r R code 10.12}
 max(entropies)
```

```{r R code 10.13}
 distributions[ which.max(entropies) ]
```

# practice 
* https://github.com/rmcelreath/statrethinking_winter2019/blob/master/homework/week05.pdf
* 1. Consider the data (Wines 2012) data table.These data are expert ratings of 20 different French and American wines by 9 different French and American judges. Your goal is to model score, the subjective rating assigned by each judge to each wine. I recommend standardizing it.

## read data
```{r practice 1.1}
data(Wines2012)
Wines2012 %>% View()
summary(Wines2012)
```

## standardize data by judge
```{r practice 1.2}
Wines2012 %>% group_by(judge) %>% mutate(score_norm=score/sum(score)) -> Wines2012 
Wines2012 %>% summarize(all=sum(score_norm)) # this should be one. OK.
# maybe this way?
Wines2012 %>% mutate(score_norm2=scale(score)) -> Wines2012
```

* In this first problem, consider only variation among judges and wines. 
Construct index variables of judge and wine and then use these index variables to construct a linear regression model. 
```{r practice 1.3}
# index wine
Wines2012 <- Wines2012 %>% group_by(judge) %>% mutate(wine_index=1:n()) #%>% View()
# index judge
Wines2012 <- Wines2012 %>% ungroup() %>% group_by(wine) %>% mutate(judge_index=1:n())  #%>% View()
# score_norm ~ judge*wines
## score_norm ~ a[judge.index] + b[judge.index]*[wine.index] true???
```

Justify your priors. You should end up with 9 judge parameters and 20 wine parameters.
```{r}
# check
Wines2012 %>% View() # select each judge to see wine index and vise versa for wine
# make it to list object
Wines2012.list<-list(
  score_norm=Wines2012$score_norm,
  score_norm2=Wines2012$score_norm2,
  judge_index=Wines2012$judge_index,
  wine_index=Wines2012$wine_index
)
```

Use ulam instead of quap to build this model, and be sure to check the chains for convergence. 
# score_norm got error
```{r practice 1.4}
practice.m1_score_norm<-ulam(
    alist(
         score_norm ~ dnorm( mu , sigma ) , # error
        #score_norm2 ~ dnorm( mu , sigma ) , # no error
        # mu <- a[judge.inex] +  b[judge.index]*wine.index,
        mu <- a[judge_index] + b[wine_index], 
        a[judge_index] ~ dnorm(0 , 1 ) ,
        b[wine_index] ~ dnorm(0 , 1 ) ,
        sigma ~ dexp(1)
),
data=Wines2012.list, chains=1,cores=2,iter=2000) # errors....
# 

```

```{r practice 1.5}
practice.m1_score_norm2<-ulam(
    alist(
        # score_norm ~ dnorm( mu , sigma ) , # error
        score_norm2 ~ dnorm( mu , sigma ) , # no error
        # mu <- a[judge.inex] +  b[judge.index]*wine.index,
        mu <- a[judge_index] + b[wine_index], 
        a[judge_index] ~ dnorm(0 , 1 ) ,
        b[wine_index] ~ dnorm(0 , 1 ) ,
        sigma ~ dexp(1)
),
data=Wines2012.list, chains=1,cores=2,iter=1000)
# 

```



If you’d rather build the model directly in Stan or PyMC3, go ahead. I just want you to use Hamiltonian Monte Carlo instead of quadratic approximation.

How do you interpret the variation among individual judges and indi- vidual wines? Do you notice any patterns, just by plotting the differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/ best on average?
* score_norm with errors


```{r}
traceplot(practice.m1_score_norm )
```

```{r}
plot(precis(practice.m1_score_norm2,2))
```


```{r}
pairs(practice.m1_score_norm2)

png(file="practice.m1_score_norm2.png",width=20,height=20,units="in",res=100)
pairs(practice.m1_score_norm2)
dev.off()
```

```{r}
traceplot(practice.m1_score_norm2 )
```

## 2. Now consider three features of the wines and judges:
* (1) flight: Whether the wine is red or white.
* (2) wine.amer: Indicator variable for American wines. 
* (3) judge.amer: Indicator variable for American judges.
* Use indicator or index variables to model the influence of these features on the scores. Omit the individual judge and wine index variables from Problem 1. Do not include interaction effects yet. Again use ulam, justify your priors, and be sure to check the chains. What do you conclude about the differences among the wines and judges? Try to relate the results to the inferences in Problem 1.
# indexing
```{r}
Wines2012.list$flight_index<-ifelse(Wines2012$flight=="white",1,2)
Wines2012.list$wine_amer_index<-ifelse(Wines2012$wine.amer==0,1,2)
Wines2012.list$judge_amer_index<-ifelse(Wines2012$judge.amer==0,1,2)
```

```{r practice 2.1}
practice.m2_score_norm2<-ulam(
    alist(
        score_norm2 ~ dnorm( mu , sigma ) , # no error
        mu <- a[flight_index] + b[wine_amer_index] + c[judge_amer_index], 
        a[flight_index] ~ dnorm(0 , 1),
        b[wine_amer_index] ~ dnorm(0 , 1),
        c[judge_amer_index] ~ dnorm(0,1),
        sigma ~ dexp(1)
),
data=Wines2012.list, chains=1,cores=2,iter=2000) # errors....
# 

```

```{r}
plot(precis(practice.m2_score_norm2,2))
```
```{r}
pairs(practice.m2_score_norm2)
png(file="practice.m2_score_norm2.png",width=20,height=20,units="in",res=100)
pairs(practice.m2_score_norm2)
dev.off()
```

```{r}
traceplot(practice.m2_score_norm2 )
```


## 3. Now consider two-way interactions among the three features.You should end up with three different interaction terms in your model. These will be easier to build, if you use indicator variables. Again use ulam, justify your priors, and be sure to check the chains. Explain what each interaction means. Be sure to interpret the model’s predictions on the outcome scale (mu, the expected score), not on the scale of individual parameters. You can use link to help with this, or just use your knowledge of the linear model instead.
* What do you conclude about the features and the scores? Can you relate the results of your model(s) to the individual judge and wine inferences from Problem 1?
### needs to learn how to do with interaction .... help me! which page in the textbook?
```{r practice 3.1, eval=FALSE}
practice.m3_score_norm2_interaction<-ulam(
    alist(
        score_norm2 ~ dnorm( mu , sigma ) , # no error
        mu <- a[flight_index] + b[wine_amer_index] + c[judge_amer_index] + d[flight_index]*[wine_amer_index]+e[wine_amer_index]*[judge_amer_index] + f[judge_amer_index]*[flight_index], 
        a[flight_index] ~ dnorm(0 , 1),
        b[wine_amer_index] ~ dnorm(0 , 1),
        c[judge_amer_index] ~ dnorm(0,1),
        d[flight_index]*[wine_amer_index] ~ dnorm(0,1),
        e[wine_amer_index]*[judge_amer_index] ~ dnorm(0,1),
        f[judge_amer_index]*[flight_index] ~ dnorm(0,1),
        sigma ~ dexp(1)
),
data=Wines2012.list, chains=1,cores=2,iter=2000) # errors....
# 

```

```{r}
link() #...
```


# 10.2. Generalized inear models
##### under construction ####


